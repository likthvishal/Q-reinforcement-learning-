{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q* Learning with FrozenLake \n",
    "\n",
    "The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, so you won't always move in the direction you intend (stochastic environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions :  4\n",
      "State Size :  16\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "print(\"Actions : \", action_size)\n",
    "print(\"State Size : \",state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 150000\n",
    "learning_rate = 0.8\n",
    "max_steps = 50\n",
    "gamma = 0.98 #discount factor\n",
    "\n",
    "epsilon = 0.35\n",
    "# max_epsilon = 1.0\n",
    "# min_epsilon = 0.01\n",
    "# decay_rate = 0.005\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Score  -43.086\n",
      "Final ? 7752 142149 150000\n",
      "CPU times: user 25.4 s, sys: 428 ms, total: 25.8 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "rewards = []\n",
    "\n",
    "seen_final = 0\n",
    "not_seen_final = 0\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    total_rewards = 0\n",
    "\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        if exp_tradeoff > epsilon:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward==0 :\n",
    "            total_rewards = total_rewards -1\n",
    "        \n",
    "        if done and reward!=0:\n",
    "            seen_final = seen_final + 1\n",
    "            total_rewards = 1000\n",
    "        \n",
    "        elif done and reward==0:\n",
    "            not_seen_final = not_seen_final +1\n",
    "            total_rewards = -100\n",
    "        \n",
    "        q_table[state, action] = q_table[state, action] + learning_rate *(total_rewards + gamma * np.max(q_table[obs, :]) - q_table[state, action])\n",
    "          \n",
    "        state = obs\n",
    "        \n",
    "           \n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            break\n",
    "         \n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp (-decay_rate * episode) \n",
    "        \n",
    "\n",
    "        \n",
    "print(\"Avg Score \", sum(rewards)/total_episodes)\n",
    "print(\"Final ?\", seen_final, not_seen_final, total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-132.56830201, -120.95884091,  -92.32612289, -147.70908425],\n",
       "       [-170.23641561,  413.38716767, -175.85932768, -157.98028887],\n",
       "       [-167.81414008,  822.17384343, -167.69361913, -166.31822488],\n",
       "       [-103.27047418, -173.95531385, -182.36473413, -179.34098715],\n",
       "       [-109.78399652, -100.13061743, -162.14047254, -120.87136768],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [-138.14243481,  100.77635185, -175.1856893 , -106.58447198],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [-140.93848928, -135.4869285 , -100.00409247, -136.05007349],\n",
       "       [-115.85080431, -109.40887874,   77.03411806, -124.92340381],\n",
       "       [-124.43889825,  806.81602372, -125.63651535, -129.2743877 ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ],\n",
       "       [-131.91329382, -105.27672231, -125.89349978,  510.65616615],\n",
       "       [ -85.65184389, -110.68297295,  996.0626709 , -115.53203129],\n",
       "       [   0.        ,    0.        ,    0.        ,    0.        ]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Steps taken 2\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Steps taken 4\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Steps taken 3\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Steps taken 4\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "Steps taken 3\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Steps taken 3\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFF\u001b[41mH\u001b[0m\n",
      "HFFG\n",
      "Steps taken 6\n",
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Steps taken 3\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Steps taken 3\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Steps taken 1\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            env.render()\n",
    "            print(\"Steps taken\", step)\n",
    "            break\n",
    "        \n",
    "        state = obs\n",
    "env.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
