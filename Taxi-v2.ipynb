{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we'll implement an agent that plays OpenAI Taxi-V2.\n",
    "\n",
    "### How to play the Game ?\n",
    "\n",
    "The goal of this game is that our agent must pick up the passenger at one location and drop him off to the goal as fast as possible.\n",
    "\n",
    "\n",
    "### Rules\n",
    "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another.\n",
    "\n",
    "* +20 points for a successful dropoff\n",
    "* -1 points for every timestep it takes.\n",
    "* -10 points for every illegal pick-up and drop-off actions (if you don't drop the passenger in one of the 3 other locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v2\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible actions :6\n",
      "State size : 500\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "print(\"Possible actions :\" +str(action_size))\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "print(\"State size : \"+str(state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((state_size, action_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 50000\n",
    "total_test_episodes = 100\n",
    "max_steps = 99\n",
    "\n",
    "learning_rate = 0.7\n",
    "gamma = 0.618\n",
    "\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        if exp_tradeoff > epsilon : #eploit\n",
    "            action = np.argmax(qtable[state,:])\n",
    "        else: #explore\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + gamma * np.max(q_table[obs, :]) - q_table[state, action])\n",
    "        \n",
    "        state = obs\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(decay_rate * episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_rewards += reward\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            break\n",
    "        \n",
    "        state = obs\n",
    "    \n",
    "env.close()\n",
    "print('Score over time: '+ str(sum(rewards)/total_test_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for step in range(max_steps):\n",
    "    action = np.argmax(q_table[state,:])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_rewards += reward\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        rewards.append(total_rewards)\n",
    "        break\n",
    "\n",
    "    state = obs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
